name: Terraform Apply

on:
  workflow_dispatch:

jobs:
  terraform:
    runs-on: ubuntu-22.04
    steps:
    - name: Checkout code
      uses: actions/checkout@v4.1.7

    - name: Set up Terraform
      uses: hashicorp/setup-terraform@v3.1.1
      with:
        terraform_version: v1.9.0

    - name: Set up SSH keys
      env:
        SSH_PRIVATE_KEY: ${{ secrets.SSH_PRIVATE_KEY }}
        SSH_PUBLIC_KEY: ${{ secrets.SSH_PUBLIC_KEY }}
        SERVICE_ACCOUNT_KEY_FILE: ${{ secrets.SERVICE_ACCOUNT_KEY_FILE }}
      run: |
        mkdir -p ~/.ssh
        echo "$SSH_PRIVATE_KEY" > ~/.ssh/id_ed25519
        chmod 600 ~/.ssh/id_ed25519
        echo "$SSH_PUBLIC_KEY" > ~/.ssh/id_ed25519.pub
        chmod 600 ~/.ssh/id_ed25519.pub
        echo "$SERVICE_ACCOUNT_KEY_FILE" > ~/.ssh/authorized_key.json
        chmod 600 ~/.ssh/authorized_key.json

    - name: Initialize Terraform
      env:
        TF_VAR_yc_cloud_id: ${{ secrets.YC_CLOUD_ID }}
        TF_VAR_yc_folder_id: ${{ secrets.YC_FOLDER_ID }}
        TF_VAR_ssh_public_key: ${{ secrets.SSH_PUBLIC_KEY }}
        TF_VAR_yc_service_account_id: ${{ secrets.YC_SERVICE_ACCOUNT_ID }}
        TF_VAR_service_account_key_file: ~/.ssh/authorized_key.json
      run: terraform init

    - name: Apply Terraform
      env:
        TF_VAR_yc_cloud_id: ${{ secrets.YC_CLOUD_ID }}
        TF_VAR_yc_folder_id: ${{ secrets.YC_FOLDER_ID }}
        TF_VAR_ssh_public_key: ${{ secrets.SSH_PUBLIC_KEY }}
        TF_VAR_yc_service_account_id: ${{ secrets.YC_SERVICE_ACCOUNT_ID }}
        TF_VAR_service_account_key_file: ~/.ssh/authorized_key.json
      run: terraform apply -auto-approve

    - name: Get Master IPs
      id: get_master_ips
      run: echo "::set-output name=ips::$(terraform output -json master_ips | jq -r '.[]')"

    - name: Get Worker IPs
      id: get_worker_ips
      run: echo "::set-output name=ips::$(terraform output -json worker_ips | jq -r '.[]')"

    - name: Install kubeadm on masters
      run: |
        for host in ${{ steps.get_master_ips.outputs.ips }}; do
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 ubuntu@$host '
            sudo apt-get update &&
            sudo apt-get install -y apt-transport-https ca-certificates curl gpg &&
            sudo mkdir -p -m 755 /etc/apt/keyrings &&
            curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg &&
            echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list &&
            sudo apt-get update &&
            sudo apt-get install -y kubelet kubeadm kubectl &&
            sudo apt-mark hold kubelet kubeadm kubectl
          '
        done

    - name: Initialize kubeadm on first master
      run: |
        ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 ubuntu@${{ steps.get_master_ips.outputs.ips[0] }} 'sudo kubeadm init --pod-network-cidr=10.244.0.0/16'

    - name: Join other masters
      run: |
        JOIN_COMMAND=$(ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 ubuntu@${{ steps.get_master_ips.outputs.ips[0] }} 'sudo kubeadm token create --print-join-command')
        for host in ${{ steps.get_master_ips.outputs.ips[1] }} ${{ steps.get_master_ips.outputs.ips[2] }}; do
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 ubuntu@$host "$JOIN_COMMAND --control-plane"
        done

    - name: Install kubeadm on workers
      run: |
        for host in ${{ steps.get_worker_ips.outputs.ips }}; do
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 ubuntu@$host '
            sudo apt-get update &&
            sudo apt-get install -y apt-transport-https ca-certificates curl gpg &&
            sudo mkdir -p -m 755 /etc/apt/keyrings &&
            curl -fsSL https://pkgs.k8s.io/core:/stable:/v1.30/deb/Release.key | sudo gpg --dearmor -o /etc/apt/keyrings/kubernetes-apt-keyring.gpg &&
            echo "deb [signed-by=/etc/apt/keyrings/kubernetes-apt-keyring.gpg] https://pkgs.k8s.io/core:/stable:/v1.30/deb/ /" | sudo tee /etc/apt/sources.list.d/kubernetes.list &&
            sudo apt-get update &&
            sudo apt-get install -y kubelet kubeadm kubectl &&
            sudo apt-mark hold kubelet kubeadm kubectl
          '
        done

    - name: Join workers
      run: |
        JOIN_COMMAND=$(ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 ubuntu@${{ steps.get_master_ips.outputs.ips[0] }} 'sudo kubeadm token create --print-join-command')
        for host in ${{ steps.get_worker_ips.outputs.ips }}; do
          ssh -o StrictHostKeyChecking=no -i ~/.ssh/id_ed25519 ubuntu@$host "$JOIN_COMMAND"
        done

    - name: Clean up on failure
      if: failure()
      env:
        TF_VAR_yc_cloud_id: ${{ secrets.YC_CLOUD_ID }}
        TF_VAR_yc_folder_id: ${{ secrets.YC_FOLDER_ID }}
        TF_VAR_ssh_public_key: ${{ secrets.SSH_PUBLIC_KEY }}
        TF_VAR_yc_service_account_id: ${{ secrets.YC_SERVICE_ACCOUNT_ID }}
        TF_VAR_service_account_key_file: ~/.ssh/authorized_key.json
      run: terraform destroy -auto-approve

    - name: Display Success Message
      if: success()
      run: echo "ВСЕ НОРМАЛЬНО"

    - name: Destroy Infrastructure
      if: always()
      env:
        TF_VAR_yc_cloud_id: ${{ secrets.YC_CLOUD_ID }}
        TF_VAR_yc_folder_id: ${{ secrets.YC_FOLDER_ID }}
        TF_VAR_ssh_public_key: ${{ secrets.SSH_PUBLIC_KEY }}
        TF_VAR_yc_service_account_id: ${{ secrets.YC_SERVICE_ACCOUNT_ID }}
        TF_VAR_service_account_key_file: ~/.ssh/authorized_key.json
      run: terraform destroy -auto-approve

    - name: Clean up SSH keys
      if: always()
      run: |
        rm -f ~/.ssh/id_ed25519
        rm -f ~/.ssh/id_ed25519.pub
        rm -f ~/.ssh/authorized_key.json
